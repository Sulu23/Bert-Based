# evaluation

from sklearn import metrics

def evaluate(true_labels, predicted_labels):

    # Use confusion matrix generated by the provided function to calculate evaluation metrics
    confusion_matrix = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels)
    
    # Precision, recall and F1 score per class
    precision = metrics.precision_score(true_labels, predicted_labels, average=None) 
    recall = metrics.recall_score(true_labels, predicted_labels, average=None) 
    f1 = metrics.f1_score(true_labels, predicted_labels, average=None)
    
    # Macro average scores
    precision_macro = metrics.precision_score(true_labels, predicted_labels, average='macro')
    recall_macro = metrics.recall_score(true_labels, predicted_labels, average='macro')
    f1_macro = metrics.f1_score(true_labels, predicted_labels, average='macro')
    
    print('***** Evaluation *****')
    print("Precision:", precision, "Recall:", recall, "F1 score:", f1, "Precision macro average:", precision_macro, "Recall macro average:", recall_macro, "F1 score macro average:", f1_macro)

